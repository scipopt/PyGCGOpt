{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring DW Dual Bounds\n",
    "A main feature of GCG is its plugin interface which can be used to plug into different parts of the solving process. In this tutorial we will take a look at how to implement a simple detector plugin. In GCG, detectors are executed before the solving process to find structures in the model passed to the solver. For more details in detectors, please consult GCG's documentation.\n",
    "\n",
    "The example used in this tutorial is based on a paper by Witt et al. We will built a detector in Python that detects all possible decompositions for a given model. After that, we will run a simple experiment in collect statistics using the Python interface. To follow along with this tutorial interactively, please download the Jupyter notebook and all referenced resources\n",
    "using the following link: [alldecomps_complete.zip](alldecomps_complete.zip)\n",
    "\n",
    "## Preparation\n",
    "We start with adding all neccessary imports as well as a utility function to return the powerset of a list. This will be usefull later since reformulating each set of constraints in the powerset is equivalent to obtaining all decompositions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[set(), {0}, {1}, {2}, {0, 1}, {0, 2}, {1, 2}, {0, 1, 2}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pygcgopt import GCGModel, Detector, SCIP_PARAMSETTING, SCIP_STAGE\n",
    "\n",
    "from itertools import chain, combinations\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "def powerset(iterable):\n",
    "    \"\"\"Returns the powerset of the passed iterable.\"\"\"\n",
    "    s = list(iterable)\n",
    "    return map(set, chain.from_iterable(combinations(s, r) for r in range(len(s)+1)))\n",
    "\n",
    "list(powerset([0,1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Detector\n",
    "To implement a detector for GCG in Python, you need to subclass the `Detector` class. It has several methods that can be overriden to plugin at different stages of the detector loop. For more details on the other methods, please consult the documentation of the Python interface. For general information on the detector loop, please consult GCG's documentation. The callbacks of the Python interface match those of the C interface.\n",
    "\n",
    "Without further ado, here is the code for the detector. We will take a look at the different parts of it afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllDecompsDetector(Detector):\n",
    "    def __init__(self, n_conss):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conss_powerset = powerset(range(n_conss))\n",
    "        self.iteration_idx = 0\n",
    "\n",
    "    def propagatePartialdec(self, detprobdata, workonpartialdec):\n",
    "        all_conss = sorted(workonpartialdec.getOpenconss(), key=lambda cons: cons.name)\n",
    "\n",
    "        new_decs = []\n",
    "        for reform_conss_idx in self.conss_powerset:\n",
    "            reform_conss = set(all_conss[i] for i in reform_conss_idx)\n",
    "            master_conss = set(all_conss) - reform_conss\n",
    "            assert len(master_conss) + len(reform_conss) == len(all_conss)\n",
    "\n",
    "            new_dec = workonpartialdec.copy()\n",
    "            new_dec.fixConssToMaster(master_conss)\n",
    "            new_dec.fixConssToBlock(reform_conss, 0)\n",
    "\n",
    "            new_decs.append(new_dec)\n",
    "\n",
    "            # print(f\"Produced decomposition at index {self.iteration_idx}: {reform_conss}\")\n",
    "            self.last_decomp = list(c.name for c in reform_conss)\n",
    "\n",
    "            self.iteration_idx += 1\n",
    "\n",
    "            # The unconditional break is intentional, read on to find out why.\n",
    "            break\n",
    "\n",
    "        \n",
    "        return {\n",
    "            \"newpartialdecs\": [new_dec]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above class is already enough to define our detector!\n",
    "\n",
    "Let's walk it through:\n",
    "\n",
    "In the constructor, we compute the powerset based on the number of constraints our model has. Each element of the `conss_powerset` list will be a *set* of constraints to reformulate.\n",
    "\n",
    "The `propagatePartialdec` method is one of three methods that can be overriden for a detector. It receives two arguments `detprobdata` of type `DetProbData` and `workonpartialdec` of type `PartialDecomposition`. The first argument contains generic information accumulated during the detection and classification process. The second argument contains a partial decomposition that our detector *may* use to derive new (partial) decompositions. In our case, the passed partial decomposition will always be empty due to how we will setup the experiment later.\n",
    "\n",
    "Our detector gets all constraints from the partial decomposition and then iterates over the previously generated powerset of constraints. We select the constraints to reformulate and the constraints to remain in the master problem. Then, we copy the partial decomposition and assign the master and reformulation constraints. In the end, we return all new partial decompositions.\n",
    "\n",
    "Note: You may wonder why we use a for-loop to iterate over the powerset but exit after one iteration. Down below, we will create a fresh `Model` for *every* decomposition and assign the *same* instance of the detector. GCG will call the `propagatePartialdec` method once for each model and our code will return a different decomposition each time because `conss_powerset` is a Python iterable object. Thus, the iteration will *not* start over but ascend every time.\n",
    "\n",
    "## Experiment\n",
    "Now that we have our detector, we want to use it to replicate the study referanced above.\n",
    "\n",
    "### Setting up the Model\n",
    "The goal is to solve the LP relaxation of the RMP of *every* reformulation. For that, we need to disable cutting planes, limit to number of nodes, and prevent GCG from aborting pricing *before* the RMP is optimal. We create a function to create a fresh model and make these settings.\n",
    "\n",
    "Note: The parameter `limits/nodes` is not a GCG parameter but a SCIP parameter. We can set using the appropriate method of PySCIPOpt's `Model` class. You can set any SCIP or GCG parameter in a similar manner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    m = GCGModel()\n",
    "\n",
    "    for det in m.listDetectors():\n",
    "        m.setDetectorEnabled(det, False)\n",
    "        m.setDetectorFinishingEnabled(det, False)\n",
    "        m.setDetectorPostprocessingEnabled(det, False)\n",
    "\n",
    "    m.setGCGSeparating(SCIP_PARAMSETTING.OFF)\n",
    "    m.setLongintParam(\"limits/nodes\", 1)\n",
    "    m.setBoolParam(\"pricing/masterpricer/abortpricingint\", False)\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciating the Detector\n",
    "Next, we will specify our problem instance. We will use a small coloring instance from the paper containing only 6 constraints for the sake of example.\n",
    "\n",
    "The problem is instanciated once to obtain the number of constraints. This information is used to instanciate our detector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'problem_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cb/51f1fs7j7cb2zcmf5bjb10zw0000gn/T/ipykernel_41689/514183256.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"results/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresults_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogs_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"log\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlogs_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'problem_name' is not defined"
     ]
    }
   ],
   "source": [
    "instance_name = \"coloring3\"\n",
    "problem_path = f\"alldecomps_instances/{instance_name}.lp.gz\"\n",
    "problem_name = Path(problem_path).stem\n",
    "\n",
    "m = init_model()\n",
    "m.readProblem(problem_path)\n",
    "n_conss = m.getNConss()\n",
    "m.freeProb()\n",
    "\n",
    "all_decomps_detector = AllDecompsDetector(n_conss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Experiment\n",
    "Now comes the fun part, we can run our little experiment!\n",
    "\n",
    "While the following code might look terrifying at first, you will notice that most of it is just boilerplate code to setup logging and to collect statistics. The important line is the one containing the call to `m.includeDetector()`. This registers our detector to GCG which will use it in addition to its predefined detectors. In our case, we disabled the other detectors when creating the model and, therefore, GCG will only use our detector.\n",
    "\n",
    "The general procedure is as follows:  \n",
    "1. Create a fresh model using `init_model()`  \n",
    "2. Include our detector  \n",
    "3. Read in our problem instance  \n",
    "4. Detect and solve the problem (we explicitely call `detect()` to avoid presolving the problem, see documentation for details)  \n",
    "5. Collect and store statistics  \n",
    "\n",
    "Running the experiment will take a few minutes, don't panic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = Path(\"results/\").joinpath(problem_name)\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "logs_dir = results_dir.joinpath(\"log\")\n",
    "logs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "current_timestamp = datetime.utcnow().strftime(\"%Y-%m-%dT%H%M%S\")\n",
    "results_file = results_dir.joinpath(f\"result_{problem_name}_{current_timestamp}.jsonl\").resolve()\n",
    "\n",
    "for i in range(2**n_conss):\n",
    "    log_path = logs_dir.joinpath(f\"log_{problem_name}_{current_timestamp}_idx_{i:06}.log\").resolve().as_posix()\n",
    "\n",
    "    m = init_model()\n",
    "    m.setLogfile(log_path)\n",
    "    m.includeDetector(all_decomps_detector, \"all_decomps_detector\", \"a\", \"Detects the power set of constraints\")\n",
    "    m.readProblem(problem_path)\n",
    "\n",
    "    m.detect()\n",
    "\n",
    "    assert(len(m.listDecompositions()) == 1)\n",
    "\n",
    "    m.optimize()\n",
    "\n",
    "    mp = m.getMasterProb()\n",
    "\n",
    "    result = {\n",
    "        \"reformulation_constraints\": all_decomps_detector.last_decomp,\n",
    "        \"iteration_idx\": i,\n",
    "        \"dual_bound\": m.getDualbound(),\n",
    "        \"total_time\": m.getTotalTime(),\n",
    "        \"solving_time\": m.getSolvingTime(),\n",
    "        \"reading_time\": m.getReadingTime(),\n",
    "        \"presolving_time\": m.getPresolvingTime(),\n",
    "        \"status\": m.getStatus(),\n",
    "        \"log_filename\": log_path,\n",
    "    }\n",
    "\n",
    "    with results_file.open(\"a\") as f:\n",
    "        json.dump(result, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "    m.freeProb()\n",
    "\n",
    "print(\"Finished experiment!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "With that we finished our little experiment and obtained the dual bounds of all possible reformulations. In a next step, we would evaluate the data that is stored in the results file."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "pygcgopt-env-docs",
   "language": "python",
   "name": "pygcgopt-env-docs"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
